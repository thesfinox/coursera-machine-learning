{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "_(by Standford University)_\n",
    "\n",
    "**WEEK 10**\n",
    "\n",
    "Course of introduction to machine learning offered by Standford University on [Coursera.org](https://www.coursera.org/learn/machine-learning). These are **notes** and **comments** based on lectures and assignments.\n",
    "\n",
    "The IPython kernel of choice is *Octave* since many exercises and assignments have been devised for that language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Dataset\n",
    "\n",
    "- a realistic no. of samples can even be in the order of $10^8$ entries,\n",
    "- gradient descent is a computational problem,\n",
    "- one way to decide whether it is a good idea to use so many entries is to look whether we have high variance (overfit) if the no. of entries is reduced.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "- instead of summing over all the gradients, we use only one training sample in shuffled order,\n",
    "- the path towards the minimum is therefore stochastic and not direct,\n",
    "- **mini batch** gradient descent uses $b$ samples and is somewhat intermediate between batch gradient descent and stochastic gradient descent,\n",
    "- mini batch can outperform stochastic only if vectorized and parallelized correctly,\n",
    "- to check the convergence, we can plot the cost function every $b$ training samples,\n",
    "- sometimes slowly decreasing the learning rate might help convergence.\n",
    "\n",
    "## Online Learning\n",
    "\n",
    "- we use one sample at a time (as in SGD) to update the parameters,\n",
    "- we can discard the sample after using it,\n",
    "- it is very susceptible to the change in user preferences, poplation, etc.\n",
    "\n",
    "## Map Reduce and Parallelization\n",
    "\n",
    "- we can also parallelize the code by using partial sums of gradient descent and then sum everything together (e.g. implementations of _MapReduce_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "5.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
