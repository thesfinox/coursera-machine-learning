{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "_(by Standford University)_\n",
    "\n",
    "**WEEK 6**\n",
    "\n",
    "Course of introduction to machine learning offered by Standford University on [Coursera.org](https://www.coursera.org/learn/machine-learning). These are **notes** and **comments** based on lectures and assignments.\n",
    "\n",
    "The IPython kernel of choice is *Octave* since many exercises and assignments have been devised for that language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Algorithms\n",
    "\n",
    "- we usually split the dataset into training and test sets to be able to evaluate the training algorithm,\n",
    "- we then compute the cost function on the test set which gives the error associated to the training (for classification we also have the **misclassification error** given by $err(h_{\\theta}(x), y) = \\begin{cases} 1~if~h_{\\theta}(x) \\ge 0.5~and~y = 0 \\\\\n",
    "1~if~h_{\\theta}(x) < 0.5~and~y = 1 \\\\ 0~otherwise \\end{cases}$,\n",
    "- in order to evaluate different hypothesis it is best to choose also a **validation set** in order to keep the test set only for predictions,\n",
    "- **NB**: the regularization must be included during training but not in the evaluation. That is we should use $J(\\theta)$ when training on the training set but $J_{train}(\\theta)$ and $J_{cv}(\\theta)$ for the **evaluation** should not show again $\\lambda$ because it is already included in the parameters $\\theta$.\n",
    "\n",
    "## Bias and Variance\n",
    "\n",
    "- **high bias**: underfit,\n",
    "- **high variance**: overfit,\n",
    "- a good idea is to plot $J_{train}$ and $J_{cv}$ as a function of the number of samples: high bias would lead to very high train and CV error and a curve that does not change much (they underfit almost always; in this case it may help to **increase the no. of features**, add **polynomial features** or **increase lambda**), while the high variance leads to a large gap between $J_{train}$ and $J_{cv}$ since the algorithms is overfitting (in this last case it may be a good idea to **increase the no. of training samples** or a **smaller features set** or **decrease lambda**).\n",
    "\n",
    "## Classification Error\n",
    "\n",
    "- the evaluation of classification algorithms is usually tricky because often a random classifier can be more accurate than a trained algorithm (this is the case of **skewed classes**),\n",
    "- we use **precision** and **recall** to properly evaluate the classificator,\n",
    "- **precision** $= \\frac{true~positives}{predicted~positives} = \\frac{TP}{TP + FP}$,\n",
    "- **recall** $= \\frac{true~positives}{actual~positives} = \\frac{TP}{TP + FN}$,\n",
    "- there is a trade-off between precision and recall due to the decision threshold,\n",
    "- is there a way to choose the decision threshold automatically? E.g.: **F1-score** $= 2 \\frac{P R}{P + R}$\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "- collecting lots of data is **not always** a good idea but it depends on the kind of algorithms and the no. of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "5.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
