{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "_(by Standford University)_\n",
    "\n",
    "**WEEK 5**\n",
    "\n",
    "Course of introduction to machine learning offered by Standford University on [Coursera.org](https://www.coursera.org/learn/machine-learning). These are **notes** and **comments** based on lectures and assignments.\n",
    "\n",
    "The IPython kernel of choice is *Octave* since many exercises and assignments have been devised for that language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "- the cost function for NN is a generalization of the usual for classification/regression problems,\n",
    "- basically we sum the loss of all the output units: $J(\\theta) = \\sum\\limits_{k = 1}^K J_k(\\theta)$ since $h_{\\theta}(x) \\in \\mathbb{R}^K$,\n",
    "- the algorithms for its minimization is called **backpropagation algorithm**:\n",
    "\n",
    "    - for each layer we compute the error associated with the activation: $\\delta^{(l)}_j = (h_{\\theta}(x))_j - y_j = a^{(l)}_j - y_j$, where $j$ runs over the output units of the network,\n",
    "    - we then compute the error for previous layers: $\\delta^{(l-1)}_{\\hat j} = (\\Theta^{(l-1)~T} \\delta^{(l)})_{\\hat j} g'(z^{(l-1)})_{\\hat j}$, where we use element-wise multiplication between the two factors (notice that $g'(z^{(l)}) = a^{(l)} ( 1 - a^{(l)} )$,\n",
    "    \n",
    "- in general the algorithm will be:\n",
    "\n",
    "    - for each sample $(x_j, y_j)$ ($1, 2, \\dots, m$):\n",
    "    \n",
    "        - compute the **forward propagation** and compute $a^{(l)}$ $\\forall l = 1, 2, \\dots, L$,\n",
    "        - compute the error terms $\\delta^{(L)} = a^{(L)} - y$,\n",
    "        - use the result to compute $\\delta^{(l)}$ for $l = 2, 3, \\dots, L - 1$,\n",
    "        - compute $\\Delta_{ij}^{(l)} \\mapsto \\Delta_{ij}^{(l)} + a^{(l)}_j \\delta_i^{(l+1)}$  (i.e. $\\Delta^{(l)} \\mapsto \\Delta^{(l)} + \\delta^{(l)} a^{(l)~T}$,\n",
    "        - compute $D_{ij}^{(l)} = \\frac{\\Delta^{(l)}_{ij}}{m} + \\lambda \\Theta^{(l)}_{ij}$ if $j \\neq 0$, or $D_{ij}^{(l)} = \\frac{\\Delta^{(l)}_{ij}}{m}$ if $j = 0$,\n",
    "        - then $\\frac{\\partial J(\\Theta)}{\\partial \\Theta_{ij}^{(l)}} = \\Delta_{ij}^{(l)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization and Matrices\n",
    "\n",
    "Since we are computing weights of a neural network, we are dealing with matrices $\\Theta$ instead of vectors. We can however recover a vector description, suitable for computing the cost function and the minimization algorithms from the matrices using the funciton `reshape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans =\n",
      "\n",
      "   231     1\n",
      "\n",
      "ans =\n",
      "\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "\n",
      "ans =\n",
      "\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "\n",
      "ans =\n",
      "\n",
      "  1  1  1  1  1  1  1  1  1  1  1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "% notice that we can can get back a vector from a matrix\n",
    "Theta_1 = ones(10,11);\n",
    "Theta_2 = 2 * ones(10,11);\n",
    "Theta_3 = 3 * ones(1,11);\n",
    "\n",
    "ThetaVec = [Theta_1(:); Theta_2(:); Theta_3(:)];\n",
    "size(ThetaVec)\n",
    "\n",
    "theta_1 = reshape(ThetaVec(1:110),10,11);\n",
    "theta_2 = reshape(ThetaVec(111:220),10,11);\n",
    "theta_3 = reshape(ThetaVec(221:231),1,11);\n",
    "\n",
    "Theta_1 == theta_1\n",
    "Theta_2 == theta_2\n",
    "Theta_3 == theta_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking\n",
    "\n",
    "- one way to make sure that the gradient descent is really working is to use the approximation of the derivative using the incremental ratio as definition of the derivative ($\\epsilon \\simeq 10^{-4}$),\n",
    "- we then check that the approx of the gradient is almost equal to the $D$ from backpropagation.\n",
    "\n",
    "## Random Initialization\n",
    "\n",
    "- we could initialise the first $\\Theta^{(1)}_{ij})$ to zero, but that would imply $a^{(2)}_1 = a^{(2)}_2$ and $\\delta^{(2)}_1 = \\delta^{(2)}_2$, thus imposing an arbitrary value to propagate equally in the network. In fact the 0 initialisation would insert redundancy in the network,\n",
    "- usually the initial weights are randomly initialised to break the symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "5.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
