{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "_(by Standford University)_\n",
    "\n",
    "**WEEK 3**\n",
    "\n",
    "Course of introduction to machine learning offered by Standford University on [Coursera.org](https://www.coursera.org/learn/machine-learning). These are **notes** and **comments** based on lectures and assignments.\n",
    "\n",
    "The IPython kernel of choice is *Octave* since many exercises and assignments have been devised for that language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "- **underfit** = \"high bias\" (very strong preconception on the problem);\n",
    "- **overfit** = \"high variance\" (the hypothesis can fit anything given a large number of parameters but cannot generalise);\n",
    "- we could try to reduce the number of features to better address the selection of the hypothesis, but this way we are throwing away information;\n",
    "- add **regularization**: reduce magnitude of some parameters $\\theta_j$;\n",
    "- the idea of regularization is to add (usually small) correction to some of the parameters (e.g. $J(\\theta) \\mapsto J(\\theta) + \\beta_1 \\theta_3^2 + \\beta_2 \\theta_4^2$);\n",
    "- it is usually a good idea to keep the parameters $\\theta$ **small** to avoid overfitting: the trade-off between the good fit of the training set and the smallness of parameters usually help curing the overfit;\n",
    "- the regularization parameter should not be too large or it will end up penalizing also the \"correct answers\", thus failing to fit also the training set;\n",
    "- we usually treat $\\theta_0$ separately, so the regularization parameter usually acts only on $\\theta_j$ for $j = 1, 2, \\dots, n$;\n",
    "- the net effect of using $J_{reg}(\\theta) = J(\\theta) + \\lambda \\theta$ is to modify the gradient descent, thus rescaling the update of $\\theta_j$ (in particular it gets scaled by a factor $< 1$.\n",
    "\n",
    "## Regularized Linear Regression\n",
    "\n",
    "- $\\theta = (X^T X + \\lambda~\\mathrm{diag}(0, 1, 1, \\dots))^{-1} X^T y$ (the initial $0$ in the diagonal is due to $\\theta_0$ not being regularized;\n",
    "- $X^T X + \\lambda~\\mathrm{diag}(0, 1, 1, \\dots)$ is always invertible;\n",
    "\n",
    "## Regularized Logistic Regression\n",
    "\n",
    "- as in all other cases, once we are able to compute $J_{reg}(\\theta)$ and $\\nabla J_{reg}(\\theta)$ we can use the `fminunc` function in Octave to find the best parameters $\\theta$ to reproduce the training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "5.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
