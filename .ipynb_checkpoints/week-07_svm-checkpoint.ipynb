{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "_(by Standford University)_\n",
    "\n",
    "**WEEK 7**\n",
    "\n",
    "Course of introduction to machine learning offered by Standford University on [Coursera.org](https://www.coursera.org/learn/machine-learning). These are **notes** and **comments** based on lectures and assignments.\n",
    "\n",
    "The IPython kernel of choice is *Octave* since many exercises and assignments have been devised for that language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- the idea is to start from logistic regression and try to use a different cost function for $y = 0$ and $y = 1$: $J(\\theta) = C (- y^T * cost_0(\\theta^T X) - ( 1 - y ) cost_1(\\theta^T X) ) + \\frac{1}{2} \\theta^T \\theta$,\n",
    "- SVM are sometimes called _large margin classifiers_ because it will try to separate data with the maximum margin possible,\n",
    "- with $C$ very large it will be very susceptible to outliers,\n",
    "- the cost function is such that $\\theta^T X \\ge 1$ when $y = 1$ or $\\theta^T X \\le -1$ when $y = 0$,\n",
    "- the minimization problem for SVM reduces to minimizing the norm of the vector $\\theta$ (consider $C$ large and the parenthesis very small in the cost function), thus when computing $\\theta^T X = p \\vert\\vert \\theta \\vert\\vert$ (where $p = \\vert\\vert X \\vert\\vert \\cos(\\phi)$) we find that $p$ will be as large as possible in order to have $\\theta^T X \\ge 1$ when $y = 1$. Therefore the boundary will keep as much margin as possible from the samples.\n",
    "\n",
    "## Kernels\n",
    "\n",
    "- the idea of kernels is used to learn non linear decision boundaries,\n",
    "- given the features $X$ we first compute the $similarity(X, f)$ with respect with some landmark $f$ we decided,\n",
    "- the function $similarity$ (or _kernel_) can take different forms (e.g.: Gaussian kernel, etc.),\n",
    "- in particular we output $1$ is $X$ is near $f$ and $0$ if $X$ is near $f$,\n",
    "- we can then use these non linear functions to learn highly non linear decision boundaries,\n",
    "- the starting point is to create the features using the landmarks as the features themselves: $f^{(i)}_m = K(x^{(i)}, x^{(m)})$,\n",
    "- the minimization problem will therefore be $J(\\theta) = C (- y^T * cost_0(\\theta^T f) - ( 1 - y ) cost_1(\\theta^T f) ) + \\frac{1}{2} \\theta^T M \\theta$, where $M$ is kernel dependent matrix,\n",
    "- NB **use feature scaling BEFORE using the Gaussian kernel**,\n",
    "- usually if the no. of features is **larger** than the no. of samples try the \"linear kernel\" (no kernel), if the no. of samples is **huge** than try to find more features and use no kernel (or logistic regression), while if the no. of samples and features are intermediate try the Gaussian kernel,\n",
    "- neural networks are usually good alternatives but it takes longer to train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "5.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
