{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "_(by Standford University)_\n",
    "\n",
    "**WEEK 9**\n",
    "\n",
    "Course of introduction to machine learning offered by Standford University on [Coursera.org](https://www.coursera.org/learn/machine-learning). These are **notes** and **comments** based on lectures and assignments.\n",
    "\n",
    "The IPython kernel of choice is *Octave* since many exercises and assignments have been devised for that language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "- the idea is to assume that the data comes from a distribution (e.g. Gaussian) and compute its p-value,\n",
    "- with $x \\in \\mathbb{R}^n$, we consider $p(x) = \\prod\\limits_{i = 1}^n p(x^{(i)}; \\mu_i, \\sigma^2_i)$ (we are assuming reasonable independence on the data, but it is almost fine also for correlated data), and then compute the p-value (or better, we consider anomaly $p(x) < \\epsilon$, which means that we consider an anomaly when the value of the Gaussian is less than a fixed values corresponding to a certain p-value),\n",
    "- there is however a difference from a simple **supervised learning algorithm**: in **anomaly detection** the positive class is usually rare, the no. of \"types\" of anomalies are usually very large and different and the future anomalies might be completely different from the test set we are using (e.g. fraud detection, engine failures, machine monitoring, etc.),\n",
    "- the ideal situation is of course to have data coming from a normal distribution: we usually apply transformation to the data to make it look \"more Gaussian\" (e.g. $log(X)$, $\\sqrt{X}$, etc.),\n",
    "- the most common problem are anomalies with $p(x)$ similar to regular data: the idea is to add features hoping that in the new feature the anomaly value is completely off,\n",
    "- NB: in anomaly detection we fit **only the negative class** during training, because we want to recongnise the positive class from it,\n",
    "- the positive class is good to find a value of $epsilon$ to detect the anomaly,\n",
    "- we usually use the **multivariate Gaussian distribution** to better model the data and to consider also the **covariance** of the data (the effect is to squeeze, elongate and rotate the distribution): $p(x; \\mu, \\Sigma) = \\frac{1}{(2 \\pi)^{\\frac{n}{2}} \\sqrt{\\det{\\Sigma}}} \\exp\\left( -\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right)$,\n",
    "- the multivariate Gaussian is equivalent to the original $p(x) = \\prod\\limits_{i = 1}^n p(x^{(i)}; \\mu_i, \\sigma^2_i)$ if $\\Sigma$ has only $0$ in the off-diagonal terms (axis aligned),\n",
    "- the multivariate model accounts \"automatically\" for correlation between features, while in the original model you have to create them \"manually\", but it is a good idea when the no. of samples is larger than the features and the features are independent (or $\\Sigma$ will not be invertible). The original model is usually more computationally efficient and scales better though.\n",
    "\n",
    "## Recommender System\n",
    "\n",
    "### Content Based Recommendation Systems\n",
    "\n",
    "- the problem with recommender systems is that not all recommendation have \"recommendation\" by users and we want to predict the \"ratings\" of the missing values (e.g. Amazon, Netflix, etc.),\n",
    "- we build a parameter vector $\\Theta$ for \"each user\" in order to predict the \"rating\" of a feature vector $x$,\n",
    "- the minimization is then a linear regression (withouth dividing $J(\\theta)$ by $m$): we can then build $\\theta$ for each user and by summing the cost function for all of them, we can find the parameters for all users together (to improve a general \"recommendation\" and for then refine it for each user),\n",
    "\n",
    "### Collaborative Filtering\n",
    "\n",
    "- how do we get the features which are characteristics of the movie, item, etc.? We ask the users to provide $\\theta$ for us (we ask them if they like sth w.r.t. sth else),\n",
    "- inverting the previous function, we can find the reasonable features for each item,\n",
    "- the idea is basically to learn $\\theta$ in order to learn $x$ in order to learn better $\\theta$ and improve $x$, etc.,\n",
    "- we can also sum together the two optimization objectives together (features and parameters) to create a collaborative filtering algorithm (in this case we get rid of the term $x_0$ and $\\theta_0$),\n",
    "- the optimization is a **low rank matrix factorization** problem,\n",
    "- the problem is then to provide the \"recommendations\": we can in fact recommend a \"movie\" j, knowing that a user liked i by looking at a small $\\vert\\vert x^{(j)} - x^{(i)} \\vert\\vert$,\n",
    "- what happens if a user did not rate anything? We use **mean normalization**: we normalize the rating to $0$ mean such that for a user withouth ratings we will predict the average ratings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "5.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
