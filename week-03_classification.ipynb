{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "_(by Standford University)_\n",
    "\n",
    "**WEEK 3**\n",
    "\n",
    "Course of introduction to machine learning offered by Standford University on [Coursera.org](https://www.coursera.org/learn/machine-learning). These are **notes** and **comments** based on lectures and assignments.\n",
    "\n",
    "The IPython kernel of choice is *Octave* since many exercises and assignments have been devised for that language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "- typical of 0/1 decisions: spam/not spam, fraudolent/non fraudolent, etc.;\n",
    "- in this case the **classes** are typically 2: true and falso (i.e. 1 and 0);\n",
    "- **multiclass classification** deals with more classes;\n",
    "\n",
    "## Algorithms\n",
    "\n",
    "- we can use thresholds to use for instance **linear regression**. E.g.: $h_{\\theta}(x) \\ge 0.5 \\Rightarrow $ predict $1$;\n",
    "- the slope of linear regression is usually an issue because it may change the threshold even if it should not;\n",
    "- even though the samples are $0 \\le y^{(i)} \\le 1$, $h_{\\theta}(x)$ might be very different from the real values;\n",
    "- **logistic regression** is always $0 \\le h_{\\theta}(x) \\le 1$.\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "- let $g(z) = \\frac{1}{1 + e^{-z}}$, then $h_{\\theta}(x) = g(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}}$ are called **sigmoid** or **logistic functions**;\n",
    "- the interpretation is: $h_{\\theta}(x)$ is the probability that $y = 1$ given the input $x$ (that is, if $h_{\\theta}(x) = 0.7$, then I have $70%$ probability that $y_{pred} = 1$. This is usually written $h_{\\theta}(x) = \\mathrm{P}(y = 1 \\vert x; \\theta)$;\n",
    "- clearly $\\mathrm{P}(y = 0 \\vert x; \\theta) + \\mathrm{P}(y = 1 \\vert x; \\theta) = 1$;\n",
    "\n",
    "## Decision Boundaries\n",
    "\n",
    "- we still have to predict $y_{pred} \\in \\left\\{ 0, 1 \\right\\}$, thus we need to use a \"trigger\" to discretize the output (e.g.: $y = 1$ if $h_{\\theta}(x) \\ge 0.5$;\n",
    "- N.B.: $g(z) \\ge 0.5$ if $z \\ge 0$, thus $h_{\\theta}(x) \\ge 0.5$ if $\\theta^T x \\ge 0$;\n",
    "- the boundary discriminating between $y_{pred} = 0$ from $y_{pred} = 1$ is called **decision boundary**;\n",
    "- the decision boundary is a property of the **hypothesis function**, not the training set.\n",
    "\n",
    "## Cost Function (Optimization Objective)\n",
    "\n",
    "- the simple _mean squared error_ for logistic regression is a non convex function, thus it has many local minima;\n",
    "- in the case of logistic regression we use $\\mathrm{Cost}(h_{\\theta}(x), y) = \\begin{cases} -\\ln(h_{\\theta}(x))~~~if~~~y = 1 \\\\ -\\ln(1 - h_{\\theta}(x))~~~if~~~y = 0 \\end{cases}$, which makes sense since we want $0$ for the correct prediction;\n",
    "- given the definition of the cost function for logistic regression, the penalty for the wrong prediction is a very large number, since the cost function goes to $\\infty$ in the case of a totally wrong prediction;\n",
    "- notice that the cost function can be written $\\mathrm{Cost}(h_{\\theta}(x), y) = - \\frac{1}{m} \\sum\\limits_{i = 1}^m ( y^{(i)} \\ln(h_{\\theta}(x^{(i)})) - (1 - y^{(i)}) \\ln(1 - h_{\\theta}(x^{(i)})) )$ and can be derived from maximum likelihood principles;\n",
    "- we then use **gradient descent** to compute the minimum of the cost function (at the end of the day the form of the derivative is the same as in linear regression, but $h_{\\theta}(x)$ is different;\n",
    "- feature scaling can also help in this case.\n",
    "\n",
    "## Optimization\n",
    "\n",
    "- there are more sophisticated optimization algorithms which are usually faster than gradient descent and which do not need to manually choose the learning rate (e.g. **BFGS**, **L-BFGS**, etc.), but they are quite complex;\n",
    "- Octave has very good implementation of minimization functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt_theta =\n",
      "\n",
      "   5.0000\n",
      "   5.0000\n",
      "\n",
      "f_val =    7.8886e-31\n",
      "exit_flag =  1\n"
     ]
    }
   ],
   "source": [
    "% build the cost function j(theta) = (theta_1 - 5)^2 + (theta_2 - 5)^2\n",
    "function [j, grad] = CostFunction(theta)\n",
    "    j       = (theta(1) - 5)^2 + (theta(2) - 5)^2;\n",
    "    grad    = zeros(2,1);\n",
    "    grad(1) = 2 * (theta(1) - 5);\n",
    "    grad(2) = 2 * (theta(2) - 5);\n",
    "end\n",
    "    \n",
    "% call the minimization function\n",
    "options  = optimset('GradObj', 'on', 'MaxIter', '100');\n",
    "in_theta = zeros(2,1);\n",
    "[opt_theta, f_val, exit_flag] = fminunc(@CostFunction, in_theta, options) % the @ sign is a pointer to the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "\n",
    "- we have more than $0$ and $1$ classes;\n",
    "- **one-vs-all** (or **one-vs-rest**) works by turning the $n$ class problem into $n$ binary classification problems: we train $n$ different classifiers capable of distinguish the classes and then we pick the classifier with the maximum value for the prediction of the class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "5.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
