{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "_(by Standford University)_\n",
    "\n",
    "**WEEK 8**\n",
    "\n",
    "Course of introduction to machine learning offered by Standford University on [Coursera.org](https://www.coursera.org/learn/machine-learning). These are **notes** and **comments** based on lectures and assignments.\n",
    "\n",
    "The IPython kernel of choice is *Octave* since many exercises and assignments have been devised for that language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Algorithms\n",
    "\n",
    "- data in unlabeled,\n",
    "- try to find structure in data.\n",
    "\n",
    "## K-Means Clustering\n",
    "\n",
    "- group data in clusters,\n",
    "- initialization is usually done by choosing centroids as random data samples from the set: $\\mu_k = x^{(k)}$ randomly selected,\n",
    "- we then compute the nearest cluster for each sample,\n",
    "- then we update the position of the centroids as the mean of the set of samples in each cluster and repeat,\n",
    "- we are minimizing $J(\\theta) = \\frac{1}{2m} \\vert\\vert \\theta^T X - \\mu \\vert\\vert^2$,\n",
    "- choosing the right no. of clusters is tricky. Usually a good way is to choose the no. of clusters at the \"elbow\" of $J(\\theta)$ as a function of the no. of clusters (clearly it is always **decreasing**, but at some point choosing more clusters is pointless).\n",
    "\n",
    "## Dimensionality Reduction (PCA)\n",
    "\n",
    "- we want to reduce the no. of features we consider or we are trying to better visualise the data (flat everything to 2D or 3D),\n",
    "- the idea is to find a lower dimensional hypersurface such that the **projection error** is minimum (that is the distance between the point and the surface is as small as possible,\n",
    "- NB **PCA is not linear regression**: linear regression minimizes the \"vertical displacement of the points $(y - h(x))^2$, while PCA is minimizing the distance between the surface and the points $\\vert\\vert \\vec{y} - (\\hat{u} \\cdot \\vec{y}) \\hat{u} \\vert\\vert$,\n",
    "- we need to scale and centre the features $x_j \\mapsto \\frac{x_j - \\mu}{s}$,\n",
    "- the vectors which identify the surface are the eigenvectors (the columns) of the covariance matrix $\\Sigma = \\frac{1}{m} \\sum\\limits_{i = 1}^m x^{(i)} x^{(i)~T} = \\frac{1}{m} X^T X$, where $X$ has $x^{(i)}$ in the rows, and $U \\in \\mathbb{R}^{n \\times n}$ is the matrix with the eigenvectors in the columns,\n",
    "- we then pick (the first) $k$ columns to reduce the dimensionality and get $z = U_{red}^T x \\in \\mathbb{R}^k$, where $x \\in \\mathbb{R}^n$ where $U_{red} \\in \\mathbb{R}^{n \\times k}$,\n",
    "- we can then go back since $z = U_{red}^T x$, then $x_{approx} = U_{red} z$,\n",
    "- in order to choose the no. of features to keep (the $k$) there are some rule of thumbs, e.g. $1 - \\frac{\\vert\\vert X - X_{\\approx} \\vert\\vert^2}{\\vert\\vert X \\vert\\vert^2} \\ge 0.99$, where 0.99 is the variance retained in the model,\n",
    "- one way to do that is to look at the $S$ matrix in $[U, S, V] = svd(\\Sigma)$, since the variance is $\\frac{\\sum\\limits_{i = 1}^k S_{ii}}{\\sum\\limits_{i = 1}^n S_{ii}}$. We can then just increase $k$ and find the right no.\n",
    "- we can use PCA to speed up some algorithms,\n",
    "- NB **scaling and $U_{red}$** must be computed only on the training set and then applied to the test set,\n",
    "- it should not be used to avoid overfitting by reducing the features: it is not how to address overfitting,\n",
    "- in any case the first step should be to first use the learning algorithm without PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "5.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
