{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "_(by Standford University)_\n",
    "\n",
    "**WEEK 2**\n",
    "\n",
    "Course of introduction to machine learning offered by Standford University on [Coursera.org](https://www.coursera.org/learn/machine-learning). These are **notes** and **comments** based on lectures and assignments.\n",
    "\n",
    "The IPython kernel of choice is *Octave* since many exercises and assignments have been devised for that language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Features\n",
    "\n",
    "### Notations\n",
    "\n",
    "- $x$ is the full feature matrix;\n",
    "- $x^{(i)}$ labels the i-th feature vector;\n",
    "- $x^{(i)}_j$ labels the j-th component of the i-th feature vector;\n",
    "- $\\theta_j$ is the j-th component of the parameters vector;\n",
    "- by definition $x^{(i)}_0 = 1$ $\\forall i = 1, 2, \\dots, m$ such that $h_{\\theta}(x) = \\theta^T x$ (that is, we add one component to couple $\\theta_0$, i.e. the intercept of the linear regression);\n",
    "- the **cost function** now becomes $J(\\theta) = \\frac{1}{2 m} \\sum\\limits_{i = 1}^m ( h_{\\theta}( x^{(i)} ) - y^{(i)} )^2$;\n",
    "- the **gradient descent** now reads $\\theta_j' = \\theta_j - \\frac{\\alpha}{m} \\sum\\limits_{i = 1}^m ( h_{\\theta}( x^{(i)} ) - y^{(i)} ) x^{(i)}_j$.\n",
    "\n",
    "### Convergence and Scaling\n",
    "\n",
    "- **gradient descent** can converge quickly if the features are on a similar **scale** (graphically: the level curves of the cost functions are wider);\n",
    "- we roughly want to have all features in $-1 \\le x_j \\le 1$. The way we do it is by rescaling $x_j \\mapsto \\frac{x_j - \\mu_j}{s_j}$ ($s_j$ is the range of variation of the feature $x_j$);\n",
    "- the **cost function** should decrease after every iteration (we can stop at the plateau, e.g. when the difference between iteration is smaller than a small parameter $\\epsilon$);\n",
    "- a crescent or oscillating $J(\\theta)$ are usually a hint that the **learning rate** $\\alpha$ should be smaller (notice that if $\\alpha$ is too small, than the convergence will require a lot of time) since for appropriate $\\alpha$, $J(\\theta)$ should be always decreasing.\n",
    "\n",
    "### Features Selection and Polynomial Regression\n",
    "\n",
    "- given my features, I can create new features as combinations of the given input;\n",
    "- with the same idea in mind, I can use polynomial models to fit my data (notice that **feature scaling** becomes increasingly important in this case)\n",
    "\n",
    "### Normal Equation\n",
    "\n",
    "- alternative to **gradient descent** (does not need iteration);\n",
    "- find the minimum of the cost function for linear regression analytically: $\\theta = ( X^T X )^{-1} X^T y$, where $X$ is the **design matrix** made of the feature verctors on the lines;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =\n",
      "\n",
      "     1   145   165\n",
      "     1   323   123\n",
      "     1   345   234\n",
      "\n",
      "y =\n",
      "\n",
      "   3\n",
      "   5\n",
      "   6\n",
      "\n",
      "th =\n",
      "\n",
      "   0.0800696\n",
      "   0.0127647\n",
      "   0.0064791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "% given a matrix X and a vector y, we can compute the normal equation\n",
    "X = [ 1, 145, 165; 1, 323, 123; 1, 345, 234 ]\n",
    "y = [ 3; 5; 6 ]\n",
    "\n",
    "th = pinv( X' * X ) * X' * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it may usually be a good idea to revert to **gradient descent** if the no. of features is $\\ge 10000$, since $X^T X \\in \\mathbb{M}^{n \\times n}( \\mathbb{R} )$ is a large matrix to invert (and inversion is an algorithm which scales as $O(n^3)$ in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "5.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
